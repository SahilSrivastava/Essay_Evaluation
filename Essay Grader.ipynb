{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text(essay):\n",
    "    essay=str(essay)\n",
    "    result = re.sub(r'http[^\\s]*','',essay)\n",
    "    # Replace sentence that is http followed till non-whitespace character, with blank space.\n",
    "    # [^abc] means Find any character NOT between the brackets\n",
    "    # \\s means whitespace character\n",
    "    # [^\\s] means Find any non-whitespace NOT between the brackets\n",
    "    result = re.sub('[0-9]+','', result).lower()\n",
    "    # Replace any number between 0-9. + mean one or more\n",
    "    result = re.sub('@[a-z0-9]+', '', result)\n",
    "    return re.sub('[%s]*' % string.punctuation, '',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello  cool asft \n"
     ]
    }
   ],
   "source": [
    "print(clean_text('hello http:\\\\www.yahoo.com cool asf98t 123'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will remove any emojis in essay\n",
    "def deEmojify(essay):\n",
    "    return essay.encode('ascii', 'ignore').decode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello \n"
     ]
    }
   ],
   "source": [
    "print(deEmojify('hello ðŸ˜ðŸ˜ðŸ˜ðŸ˜ðŸ˜ðŸ˜ðŸ˜ðŸ˜'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate number of characters in a  essay\n",
    "def char_count(essay):\n",
    "    clean_essay = re.sub(r'\\s', '', str(essay).lower())\n",
    "    #Remove any whitespace\n",
    "    return len(clean_essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "print(char_count('hello I am good'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sahil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to calculate number of words in a  essay\n",
    "def word_count(essay):\n",
    "    clean_essay = re.sub(r'\\W', ' ', essay)\n",
    "    # Replace a non-word character with space\n",
    "    # \\W  mean to find a non-word character\n",
    "    words = nltk.word_tokenize(clean_essay)\n",
    "    # A sentence or data can be split into words using the method\n",
    "    return len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "print(word_count(\"All work ðŸ˜ and no play makes jack a dull boy, all work and no play\"))\n",
    "# 'a' and ðŸ˜ is not a token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_count(essay):\n",
    "    sentences = nltk.sent_tokenize(essay)\n",
    "    return len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(sent_count('All work and no play makes jack dull boy. All work and no play makes jack a dull boy. I am, good'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return average length of words in sentence\n",
    "def avg_word_len(essay):\n",
    "    clean_essay = re.sub(r'\\W', ' ', essay)\n",
    "    words = nltk.word_tokenize(clean_essay)\n",
    "    #z = 0\n",
    "    #for word in words:\n",
    "    #    z = z + len(word)\n",
    "    #print(z/len(words))\n",
    "    return sum(len(word) for word in words) / len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4210526315789473\n"
     ]
    }
   ],
   "source": [
    "print(avg_word_len(\"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get tokenize words in a sentence\n",
    "def get_wordlist(sentence): \n",
    "    clean_sentence = re.sub(\"[^A-Z0-9a-z]\",\" \", sentence)\n",
    "    # Replace non-alphanumeric with space.\n",
    "    wordlist = nltk.word_tokenize(clean_sentence)\n",
    "    return wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['All', 'work', 'and', 'no', 'play', 'makes', 'jack', 'dull', 'boy']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_wordlist('All?work and no#play makes jack dull boy.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize whole essay into words.\n",
    "def tokenize(essay):\n",
    "    stripped_essay = essay.strip()\n",
    "    # Remove trailing and leading spaces.\n",
    "    \n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(stripped_essay)\n",
    "    # Tokenize into sentences\n",
    "    \n",
    "    tokenized_sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            tokenized_sentences.append(get_wordlist(raw_sentence))\n",
    "    \n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['All', 'work', 'and', 'no', 'play', 'makes', 'jack', 'dull', 'boy'],\n",
       " ['Goal', 'is', 'nice'],\n",
       " ['Hello', 'me'],\n",
       " ['To', 'the', 'group'],\n",
       " ['I', 'am', 'nice']]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essay = \"  All work and no play makes jack dull boy. Goal is nice. Hello me. To the group. I am nice. \"\n",
    "tokenize(essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sahil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sahil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# pos_tag require nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Function to create lemmas and return the number of lemma in essay\n",
    "def count_lemmas(essay):\n",
    "    \n",
    "    tokenized_sentences = tokenize(essay)\n",
    "    # Return word token of each sentence\n",
    "    \n",
    "    lemmas = []\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    #Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item.\n",
    "    #So it links words with similar meaning to one word.\n",
    "    #Examples of lemmatization:\n",
    "    #rocks : rock\n",
    "    #corpora : corpus\n",
    "    #better : good\n",
    "    for sentence in tokenized_sentences:\n",
    "        tagged_tokens = nltk.pos_tag(sentence)\n",
    "        # Return words in a sentence w/ POS tag \n",
    "        for token_tuple in tagged_tokens:\n",
    "            pos_tag = token_tuple[1]\n",
    "            if pos_tag.startswith('N'): \n",
    "                pos = wordnet.NOUN\n",
    "                # Lemmatize take simple pos tags. NN will be given as n and so on for others. \n",
    "                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "                # Lemmatize word w/ pos tag to get similar meaning even if words are diffrent\n",
    "            elif pos_tag.startswith('J'):\n",
    "                pos = wordnet.ADJ\n",
    "                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "            elif pos_tag.startswith('V'):\n",
    "                pos = wordnet.VERB\n",
    "                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "            elif pos_tag.startswith('R'):\n",
    "                pos = wordnet.ADV\n",
    "                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "            else:\n",
    "                pos = wordnet.NOUN\n",
    "                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))  \n",
    "    lemma_count = len(set(lemmas))\n",
    "    # Set remove repeating items\n",
    "    return lemma_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('All', 'DT'), ('work', 'NN'), ('and', 'CC'), ('no', 'DT'), ('play', 'NN'), ('makes', 'VBZ'), ('jack', 'NN'), ('dull', 'JJ'), ('good', 'JJ'), ('boy', 'NN')]\n",
      "('All', 'DT')\n",
      "DT\n",
      "n\n",
      "good\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essay = \"  All work and no play makes jack dull good boy. Goal is nice. Hello me. To the better group. I am nice. \"\n",
    "tc = tokenize(essay)\n",
    "\n",
    "\n",
    "z = nltk.pos_tag(tc[0]) \n",
    "\n",
    "print(z)\n",
    "print(z[0])\n",
    "print(z[0][1])\n",
    "\n",
    "print(wordnet.NOUN)\n",
    "\n",
    "print(WordNetLemmatizer().lemmatize('better', 'a'))\n",
    "\n",
    "count_lemmas(essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4, 5}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([1,1,2,3,4,2,4,2,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "# Function to count the number of mispell words in our essay\n",
    "def count_spell_error(essay):    \n",
    "    clean_essay = re.sub(r'\\W', ' ', str(essay).lower())\n",
    "    clean_essay = re.sub(r'[0-9]', '', clean_essay)\n",
    "    #big.txt: It is a concatenation of public domain book excerpts from Project Gutenberg and lists of most frequent words from Wiktionary and the British National Corpus.It contains about a million words.\n",
    "    data = open('big.txt').read()\n",
    "    words_ = re.findall('[a-z]+', data.lower())\n",
    "    word_dict = collections.defaultdict(lambda: 0)\n",
    "    for word in words_:\n",
    "        word_dict[word] += 1\n",
    "    clean_essay = re.sub(r'\\W', ' ', str(essay).lower())\n",
    "    clean_essay = re.sub(r'[0-9]', '', clean_essay)\n",
    "    mispell_count = 0\n",
    "    words = clean_essay.split()   \n",
    "    for word in words:\n",
    "        if not word in word_dict:\n",
    "            mispell_count += 1\n",
    "    return mispell_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_spell_error(\"I am wronf. What shoulf I do?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate number of nouns, adjectives, verbs and adverbs in an essay\n",
    "def count_pos(essay):\n",
    "    tokenized_sentences = tokenize(essay)\n",
    "    noun_count = 0\n",
    "    adj_count = 0\n",
    "    verb_count = 0\n",
    "    adv_count = 0\n",
    "    for sentence in tokenized_sentences:\n",
    "        tagged_tokens = nltk.pos_tag(sentence)\n",
    "        for token_tuple in tagged_tokens:\n",
    "            pos_tag = token_tuple[1]\n",
    "            if pos_tag.startswith('N'): \n",
    "                noun_count += 1\n",
    "            elif pos_tag.startswith('J'):\n",
    "                adj_count += 1\n",
    "            elif pos_tag.startswith('V'):\n",
    "                verb_count += 1\n",
    "            elif pos_tag.startswith('R'):\n",
    "                adv_count += 1\n",
    "    return noun_count, adj_count, verb_count, adv_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# getiing Bag of Words (BOW) counts\n",
    "def get_count_vectors(essays):\n",
    "    vectorizer=CountVectorizer(max_features=10000,ngram_range=(1,3))\n",
    "    # Convert a collection of text documents to a matrix of token counts\n",
    "    # If â€˜englishâ€™, a built-in stop word list for English is used. \n",
    "    count_vectors = vectorizer.fit_transform(essays)\n",
    "    #Learn the vocabulary dictionary and return term-document matrix.\n",
    "    #Shows which documents contain which terms and how many times they appear.\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    return feature_names, count_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12978, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataframe = pd.read_csv('essays_and_scores.csv', encoding = 'latin-1')\n",
    "data = dataframe[['essay_set','essay','domain1_score']].copy()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sahil\\Anaconda3\\envs\\essay_grader\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#splitting data into train data and test data (70/30)\n",
    "feature_names_cv,count_vectors=get_count_vectors(data[data['essay_set']==1]['essay'])\n",
    "X_cv=count_vectors.toarray()\n",
    "y_cv=data[data['essay_set']==1]['domain1_score'].as_matrix()\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_cv,y_cv,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 1.76\n",
      "Variance score: 0.32\n",
      "Cohen's kappa score: 0.17\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import numpy as np\n",
    "# Training a Linear Regression model using only Bag of Words (BOW)\n",
    "linear_regressor = LinearRegression()\n",
    "linear_regressor.fit(X_train, y_train)\n",
    "y_pred = linear_regressor.predict(X_test)\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred))\n",
    "print('Variance score: %.2f' % linear_regressor.score(X_test, y_test))\n",
    "print('Cohen\\'s kappa score: %.2f' % cohen_kappa_score(np.rint(y_pred), y_test))\n",
    "# The Kappa statistic (or value) is a metric that compares an Observed Accuracy with an Expected Accuracy (random chance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sahil\\Anaconda3\\envs\\essay_grader\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 1.06\n",
      "Variance score: 0.59\n",
      "Cohen's kappa score: 0.24\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Training a Lasso Regression model using only Bag of Words (BOW)\n",
    "alphas = np.array([3, 1, 0.3, 0.1, 0.03, 0.01])\n",
    "lasso_regressor = Lasso()\n",
    "grid = GridSearchCV(estimator = lasso_regressor, param_grid = dict(alpha=alphas))\n",
    "grid.fit(X_train, y_train)\n",
    "y_pred = grid.predict(X_test)\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred))\n",
    "print('Variance score: %.2f' % grid.score(X_test, y_test))\n",
    "print('Cohen\\'s kappa score: %.2f' % cohen_kappa_score(np.rint(y_pred), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting essay features\n",
    "def extract_features(data):\n",
    "    features = data.copy()\n",
    "    features['char_count'] = features['essay'].apply(char_count)\n",
    "    features['word_count'] = features['essay'].apply(word_count)\n",
    "    features['sent_count'] = features['essay'].apply(sent_count)\n",
    "    features['avg_word_len'] = features['essay'].apply(avg_word_len)\n",
    "    features['lemma_count'] = features['essay'].apply(count_lemmas)\n",
    "    features['spell_err_count'] = features['essay'].apply(count_spell_error)\n",
    "    features['noun_count'], features['adj_count'], features['verb_count'], features['adv_count'] = zip(*features['essay'].map(count_pos))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_set1=extract_features(data[data['essay_set']==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sahil\\Anaconda3\\envs\\essay_grader\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\sahil\\Anaconda3\\envs\\essay_grader\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# splitting data (BOW + other features) into train data and test data (70/30)\n",
    "X = np.concatenate((features_set1.iloc[:, 3:].as_matrix(), X_cv), axis = 1)\n",
    "y = features_set1['domain1_score'].as_matrix()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sahil\\Anaconda3\\envs\\essay_grader\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7102698076299961\n",
      "0.1\n",
      "Mean squared error: 0.65\n",
      "Variance score: 0.71\n",
      "Cohen's kappa score: 0.37\n"
     ]
    }
   ],
   "source": [
    "# Training a Lasso Regression model using all the features(BOW+other features)\n",
    "alphas = np.array([3, 1, 0.3, 0.1])\n",
    "lasso_regressor = Lasso()\n",
    "grid = GridSearchCV(estimator = lasso_regressor, param_grid = dict(alpha=alphas))\n",
    "grid.fit(X_train, y_train)\n",
    "y_pred = grid.predict(X_test)\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred))\n",
    "print('Variance score: %.2f' % grid.score(X_test, y_test))\n",
    "print('Cohen\\'s kappa score: %.2f' % cohen_kappa_score(np.rint(y_pred), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sahil\\Anaconda3\\envs\\essay_grader\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\sahil\\Anaconda3\\envs\\essay_grader\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# splitting data (only 10 features) into train data and test data (70/30)\n",
    "X = features_set1.iloc[:, 3:].as_matrix()\n",
    "y = features_set1['domain1_score'].as_matrix()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sahil\\Anaconda3\\envs\\essay_grader\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7158628075449887\n",
      "0.3\n",
      "Mean squared error: 0.71\n",
      "Variance score: 0.69\n",
      "Cohen's kappa score: 0.37\n"
     ]
    }
   ],
   "source": [
    "# Training a Lasso Regression model using only 10 features\n",
    "alphas = np.array([3, 1, 0.3, 0.1, 0.3])\n",
    "lasso_regressor = Lasso()\n",
    "grid = GridSearchCV(estimator = lasso_regressor, param_grid = dict(alpha=alphas))\n",
    "grid.fit(X_train, y_train)\n",
    "y_pred = grid.predict(X_test)\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred))\n",
    "print('Variance score: %.2f' % grid.score(X_test, y_test))\n",
    "print('Cohen\\'s kappa score: %.2f' % cohen_kappa_score(np.rint(y_pred), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
